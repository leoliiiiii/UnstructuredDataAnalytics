{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your name:\n",
    "\n",
    "Your Andrew ID:\n",
    "\n",
    "Collaborators (if none, say \\\"none\\\"; do *not* leave this blank):\n",
    "\n",
    "Reminder: you should not be sharing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Email spam classification [25 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - Get the data from: http://www.andrew.cmu.edu/user/georgech/preprocessed-enron-email-dataset.zip\n",
    "   - Unzip this into the same folder as this notebook, rename it to `email-data`\n",
    "   - The folder contains 3 subfolders:\n",
    "      - `ham` contains ham emails.\n",
    "      - `spam` contains spam emails.\n",
    "      - `testing` is a folder containing test emails for your classifier. The ham/spam label is in the filename.\n",
    "      \n",
    "**Important**: For this problem, do *not* use neural nets/deep nets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Print the number of ham and spam emails [1 point]**\n",
    "\n",
    "In addition to providing the code, respond to the following questions:\n",
    "\n",
    "   - Is this dataset imbalanced? Will this be problematic in training the model?\n",
    "   - If so, how would you address it? (You do *not* have to implement what you suggest here for later parts of the problem.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers to the above questions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Construct the documents [4 points]**\n",
    " \n",
    "   - Provided below is a function that returns a document present in a file given a fileName.\n",
    "   - The function performs some preprocessing to (1) remove punctuation, (2),(3) remove whitespace and (4) lowercase all words.\n",
    "   - Use this function to construct a list of documents.\n",
    "   - Also construct a list of document labels containing `1` for spam and `0` for ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import codecs\n",
    "\n",
    "def make_word_list(path):\n",
    "    \n",
    "    with codecs.open(path, \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "        corpus_text = f.read()\n",
    "\n",
    "    for c in string.punctuation:\n",
    "        corpus_text = corpus_text.replace(c, \"\")  # -- (1)\n",
    "    \n",
    "    text = re.sub(r'\\S*\\d\\S*','',corpus_text) # -- (2)\n",
    "    text = re.sub(r'[^\\w\\s]','',text)         # -- (3)\n",
    "    \n",
    "    text = text.lower().split()           # -- (4)         \n",
    "    \n",
    "    li = []\n",
    "    for token in text:\n",
    "        li.append(token)\n",
    "\n",
    "    return \" \".join(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Construct the document matrix `X` as a matrix of word frequencies [5 points]**\n",
    "\n",
    "   - Use the `CountVectorizer` from scikit-learn.\n",
    "   - Set `min_df=50`; this drops words that don't occur in at least 50 documents.\n",
    "   - Set `stop_words=\"english\"` and `max_df=0.8` to filter out stop-words.\n",
    "   - Print the size of the vocabulary (number of unique words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) k-NN and random forest: Hyperparameter selection [5 points]**\n",
    "\n",
    "Now that you have your documents and labels as training data, you can perform 5-fold cross-validation to select the hyperparameters for different learning algorithms.\n",
    "\n",
    "The hyperparameter with the best performance averaged across 5 folds is chosen. Use the **weighted F1-score** as the evaluation metric (i.e., for the `f1_score` function imported from `sklearn.metrics`, be sure to use the parameter `average='weighted'`).\n",
    "\n",
    "   - k-NN: Select `k` from a range of values of your choice.\n",
    "   - Random forest: Select `max_features` **and** `min_samples_leaf` from a grid of your choice.\n",
    "\n",
    "Store each chosen hyperparameter as `best_k`, `best_max_features`, and `best_min_samples_leaf` respectively.\n",
    "\n",
    "Provided is some seed code for cross-validation that you may modify and reuse. Do not use the cross-validations score or grid-search functions from scikit-learn (you may use `KFold`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "num_folds = 5\n",
    "k_fold = KFold(num_folds)\n",
    "hyperparameter_settings = []  # fill this with hyperparameter settings that you want to try\n",
    "\n",
    "best_hyperparam_setting = None\n",
    "best_cross_val_score = -np.inf  # assumes that a higher score is better\n",
    "for hyperparam_setting in hyperparameter_settings:\n",
    "    fold_scores = []\n",
    "    # your code to train and score the training data here\n",
    "    \n",
    "    cross_val_score = np.mean(fold_scores)\n",
    "    if cross_val_score > best_cross_val_score:  # assumes that a higher score is better\n",
    "        best_cross_val_score = cross_val_score\n",
    "        best_hyperparam_setting = hyperparam_setting\n",
    "\n",
    "print('Best hyperparameter setting:', best_hyperparam_setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Classifier testing: Precision-Recall and ROC curves [10 points]**\n",
    "\n",
    "   - Use the best hyperparameters for each classifier from the previous question to **train** your classifiers on the full training data.\n",
    "   - Use test emails in the `testing` folder to **test** your classifiers and construct the plots below.\n",
    "\n",
    "Things to plot:\n",
    "\n",
    "   - Construct one plot containing 2 ROC curves, one for each classifier. (We vary the threshold probability of declaring an email to be spam to obtain these plots.)\n",
    "   - In the legend of this plot, display the AUC for each classifier.\n",
    "   - Construct one plot containing 2 precision-recall curves, one for each classifier.\n",
    "   - In the legend of each plot, display the average precision for each classifier using the sklearn function [`average_precision_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score); average precision is essentially an approximation of the area under the precision-recall curve that avoids being overly optimistic.\n",
    "\n",
    "Note that these plots are on the test data: you will have to read in this data, construct a document matrix and labels. Some words in the test data may not have been present in the training data: there are multiple ways to address this, briefly describe your approach.\n",
    "\n",
    "Things to answer:\n",
    "\n",
    "   - Of the ROC and Precision-Recall curves, which one would you use for this task and why?\n",
    "   - Which classifier is the best, according to your chosen curves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers to the above questions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Representation Learning [40 points]\n",
    "\n",
    "Before getting started, please put the attached file `pretrained_convnet.pt` in the same directory as this notebook.\n",
    "\n",
    "Recall from lecture the \"crumpled paper analogy\": in a neural net, as you progress through the layers from input to output, the layers should be \"unfolding the original space\" to disentangle the classes. When you get to the layer before the classifier, you should've changed the data representation into something that makes classification easy. **We will refer to the layer before the classifier in the neural net as the bottleneck layer.**\n",
    "\n",
    "In this problem, you will be using an output of a bottleneck layer from pre-trained CNN as input features for a few simple classifiers. The CNN provided was trained with the Fashion MNIST dataset which consists of 10 classes of 28x28 grayscale images; this dataset is very similar to MNIST in that there are 10 classes with images that are the same size as MNIST and in fact even the training and test set sizes are the same (60,000 and 10,000 respectively). However, the classes are not the digits 0 through 9. Instead the classes are:\n",
    "\n",
    "| Class | Description |\n",
    "| ----- | :---------: |\n",
    "| 0     | T-shirt/top |\n",
    "| 1     | Trouser     |\n",
    "| 2     | Pullover    |\n",
    "| 3     | Dress       |\n",
    "| 4     | Coat        |\n",
    "| 5     | Sandal      |\n",
    "| 6     | Shirt       |\n",
    "| 7     | Sneaker     |\n",
    "| 8     | Bag         |\n",
    "| 9     | Ankle boot  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "from UDA_pytorch_utils import UDA_pytorch_classifier_fit, \\\n",
    "        UDA_plot_train_val_accuracy_vs_epoch, UDA_pytorch_classifier_predict, \\\n",
    "        UDA_pytorch_model_transform, UDA_compute_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading in the data.** We first load the Fashion MNIST dataset and plot some example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not modify this cell\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='data/',\n",
    "                                                  train=True,\n",
    "                                                  transform=transforms.ToTensor(),\n",
    "                                                  download=True)\n",
    "\n",
    "train_images = torch.tensor([image.numpy() for image, label in train_dataset])\n",
    "train_labels = torch.tensor([label for image, label in train_dataset])\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='data/',\n",
    "                                                 train=False,\n",
    "                                                 transform=transforms.ToTensor(),\n",
    "                                                 download=True)\n",
    "\n",
    "test_images = torch.tensor([image.numpy() for image, label in test_dataset])\n",
    "test_labels = torch.tensor([label for image, label in test_dataset])\n",
    "\n",
    "# show some of the first training images\n",
    "square_grid_num_rows = 8\n",
    "num_images_to_show = square_grid_num_rows ** 2\n",
    "for idx in range(num_images_to_show):\n",
    "    plt.subplot(square_grid_num_rows, square_grid_num_rows, idx + 1)\n",
    "    plt.imshow(train_images[idx][0], cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) [5 points]** Select train and test data that belong to classes 2 (corresponding to pullovers) or 6 (corresponding to fancier shirts than t-shirts; note: class 0 corresponds to t-shirts), i.e., subsample all the data that has label 2 or 6. Save the variables as `x_train`, `x_test`, `y_train`, `y_test`. From now on, we are only dealing with the images that are in classes 2 (pullovers) or 6 (shirts). Moreover, in `y_train` and `y_test`, please renumber the two classes so that pullovers now instead corresponds to 0 and shirts corresponds to 1 (we will consider shirts to be the \"positive\" class and pullovers to be the \"negative\" class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not modify this cell\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make some t-SNE visualizations shortly and using all the training data will be too slow. We reduce the size of the training dataset by randomly subsampling 1000 samples from `x_train`/`y_train` pairs and saving the subsampled versions as `x_train_small`/`y_train_small` (the i-th image in `x_train_small` should have its label given by the i-th entry in `y_train_small`). We have provided the indices of which 1000 samples to use (from within `x_train`/`y_train`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not modify this cell\n",
    "np.random.seed(0)\n",
    "random_sample = np.random.permutation(len(x_train))[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) [9 points]** We are ready to load the pre-trained convnet. The bottleneck layer is right before the final linear layer and corresponds to the output of a ReLU that has 84 values. **We refer to the output of this ReLU as the bottleneck feature vector representation of whatever the input is.**\n",
    "\n",
    "Note that the convnet was pre-trained using the entire training dataset which consists of 60,000 samples and all 10 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained CNN\n",
    "convnet = nn.Sequential(nn.Conv2d(1, 6, 3),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2),\n",
    "                        nn.Conv2d(6, 16, 3),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2),\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(in_features=16 * 5 * 5, out_features=120),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(in_features=120, out_features=84),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(in_features=84, out_features=10))\n",
    "convnet.load_state_dict(torch.load('pretrained_convnet.pt'))\n",
    "convnet.eval()\n",
    "print('[Summary of pre-trained convnet]')\n",
    "print(summary(convnet, input_data=torch.zeros((1, 1, 28, 28))))\n",
    "print()\n",
    "\n",
    "# select the pretrained CNN upto the bottleneck layer\n",
    "intermediate_layer_model = convnet[:-1]\n",
    "print('[Summary of the pre-trained convnet up to the bottleneck layer]')\n",
    "print(summary(intermediate_layer_model, input_data=torch.zeros((1, 1, 28, 28))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first extract bottleneck feature vectors of the subsampled 1000 training data from the intermediate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not modify this cell\n",
    "x_train_small_bottleneck_representation = UDA_pytorch_model_transform(intermediate_layer_model, x_train_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, visualize two different types of features with t-SNE.\n",
    "\n",
    "1. Make a t-SNE plot using the **784 (=28$\\times$28) dimensional raw features** from the subsampled training data (i.e., using `x_train_small`). Set `n_components=2, verbose=1, perplexity=25, random_state=0` for `TSNE` from scikit-learn. Plot the two classes in different colors.\n",
    "\n",
    "2. Repeat the previous step (using the exact same arguments for `TSNE`) except now using the **84 dimensional bottleneck features** (i.e., using `x_train_small_bottleneck_representation`.\n",
    "\n",
    "You may need to convert between PyTorch tensors and NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) [16 points]** We now evaluate on test data and compare several different classifiers. Train **5-NN** and **random forest** classifiers with **784 (=28$\\times$28) raw features** and, separately, the **84 bottleneck features** from **all training data**, respectively. That means you should train 4 models in total:\n",
    "\n",
    "1. 5-NN classifier trained on all raw training features\n",
    "2. 5-NN classifier trained on all training bottleneck features\n",
    "3. Random forest trained on all raw training features (for reproducibility of code, please use arguments `n_estimators=100, random_state=0` for `RandomForestClassifier`)\n",
    "4. Random forest trained on all training bottleneck features (again, use `n_estimators=100, random_state=0`)\n",
    "\n",
    "Test each model with corresponding **test dataset** and print the test accuracy. Also, compute and print the test accuracy of the **pre-trained convnet** (`convnet`) by using **test data**. You may think of this task as filling in the blanks of the following table with test accuracy results.\n",
    "\n",
    "| Model type       | Raw features        | Bottleneck features  |\n",
    "|:---------------- |:-------------------:|:--------------------:|\n",
    "| Pretrained CNN   |                     | this cell stays empty|\n",
    "| 5-NN             |                     |                      |\n",
    "| Random forest    |                     |                      |\n",
    "\n",
    "Important: the pre-trained convnet is for all 10 classes; only look at the probabilities corresponding to pullovers (class 2) and shirts (class 6), and take an argmax only over these two class probabilities to come up with predictions stricted to pullovers or shirts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the five models above, which model performed the best? Which model showed the lowest accuracy? For the 5-NN and random forest classifiers, is the prediction accuracy higher using bottleneck features instead of the raw features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answers here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) [10 points]** We now look at plotting an ROC curve, varying $k$ in $k$-NN. Train k-NN classifiers with $k=1,2,...,50$ with **bottleneck** feature vectors of all **training** data. As a result, you will get 50 classifiers. Draw a scatter plot that has false positive rate (FPR) on x-axis and true positive rate (TPR) on y-axis. Each scatter point represents one k-NN classifier. Use the **test** data to compute FPR and TPR.\n",
    "\n",
    "Recall that true and false positive rate calculations require you to specify one of the two outcomes as the \"positive\" class and the other as the \"negative\" class. For this problem, use class 6 (shirts) as the \"positive\" class, and class 2 (pullovers) as the \"negative\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you work for a company that wants to figure out when a shirt appears among photos that have either shirts or pullovers. The company tells you that it cannot tolerate a false positive rate of detecting shirts that is more than 12.5%. What value of number of nearest neighbors $k$ achieves the highest true positive rate, but doesn't have more than a 12.5% false positive rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here (if it helps you justify your answer above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Sentiment Analysis [35 points]\n",
    "\n",
    "Download data from: http://www.andrew.cmu.edu/user/georgech/HW3-data.zip <br>\n",
    "\n",
    "The folder contains:\n",
    "\n",
    "- `train.csv`\n",
    "- `test.csv`\n",
    "\n",
    "In this problem, we look at predicting whether a tweet has positive or negative sentiment.\n",
    "\n",
    "**We intentionally wrote this problem to be a bit open-ended to let you play with different neural net code yourself and also for you to compare against a classical baseline.** We will give full credit even if you subsample the training data. For example, you only work with 5000 randomly chosen training tweets instead of all of them (over 1 million tweets). Also, for the random forest part (the last part), you can keep track of only the most frequently occurring 1000 words (again, this is enough for full credit) and you can also set the RandomForestClassifier argument \"n_jobs\" to -1 to use all your CPU threads. However, do not subsample the test dataset (which is small anyways).\n",
    "\n",
    "If you want to have practice with what it's like working with a sizable dataset though, we do encourage you to try to use as much of the training data as possible (our solution code uses the full training data in an 80-20 split between a proper training set and a validation set, just like in lectures). Note that using all the data does involve more careful data preprocessing to avoid possible memory issues. Some suggestions:\n",
    "\n",
    "- Using your knowledge from weeks 1 and 2, you can write your own code to tokenize tweets and only include some of the tokens (e.g., only keeping the most common 1000 tokens, etc); in particular you can build your own vocabulary. As a suggestion, make vocabulary token \\#0 correspond to a special padding character `\"<pad>\"`; this special index 0 will in fact be automatically added by the batching process for recurrent neural nets when making different tweets within the same batch have the same length. As another suggestion, make a special vocabulary token `\"<unk>\"` for unknown tokens that you've decided not to keep track of. For example, if you only keep track of the most common 1000 tokens, then any token that is not one of these most common 1000 gets represented as `\"<unk>\"`.\n",
    "\n",
    "- For the last part, training a random forest for sentiment analysis could be very computationally expensive. If you are already subsampling the training data beforehand, then great, just work with a smaller dataset (note: you should use the same training data for both the LSTM and for the random forest so that the comparison between them is fair). If you want to still try to use the full training dataset or as much of it as possible, then we suggest that you represent the feature vectors as a sparse matrix. You might also want to set the \"max_samples\" argument to be something like 0.1 so that each tree uses a 10% subsample of the proper training set (with the default choice of 100 trees, across all your trees, you will likely use the vast majority of the full training set---it's just that each tree won't look at more than 10% of the proper training set). Also, simple data splitting suffices; cross-validation here would be very expensive. Moreover, it's fine to not train a random forest from scratch using the best hyperparameters found; instead just use whichever model was trained using the proper training data with the best hyperparameters found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) [5 points]** We first read in the tweet data. Read the data present in `train.csv` file. **Please do not change the filename. In particular, use a relative path, i.e., `./HW3-data/train.csv`**. Then do the following:\n",
    "\n",
    "1. Keep only the sentiment and sentiment text in the data - the first and the last coumn\n",
    "2. Print the number of positive and negative sentiment labels\n",
    "\n",
    "Note: If you are using `open()`, you may have to set `encoding='iso8859'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) [20 points]** Modify the LSTM demo code from lecture to work with this Twitter dataset. Play with the learning rate and batch size so that the training gives reasonable increases in training and validation accuracies as we progress through epochs (leave the number of epochs at 10). Also feel free to try different neural net architectures (although you do not have to). What test accuracy are you able to achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) [10 points]** How well does an LSTM compare with using a classical classifier? To investigate this question, let's try training a random forest for sentiment analysis. Your code from part **(b)** should involve coming up with encoded representations of text, i.e., representing a tweet using a sequence of word indices. Use this to come up with a term frequency representation for each tweet. Feel free to remove stop words and/or apply TF-IDF weighting, and then feed the resulting feature vector representations for tweets to a random forest classifier. You can use cross-validation to select hyperparameters. Try to make the random forest classifier perform as well as possible (it is good practice in real-world applications to try to make baselines as good as possible rather than intentionally using a baseline with lousy hyperparameter choices; in particular, it is *bad* practice to tune hyperparameters carefully only on a method you're proposing while not tuning hyperparameters on baselines!). What test set accuracy are you able to achieve with the random forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
